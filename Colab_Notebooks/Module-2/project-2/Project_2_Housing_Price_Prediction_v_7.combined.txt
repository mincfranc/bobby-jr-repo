<a href="https://colab.research.google.com/github/RSBalchII/cohort-15-data-science/blob/main/Project_2_Housing_Price_Prediction_v_6.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
# Project 2: Housing Price Prediction

The primary objective of this project is to investigate the relationship between various housing characteristics and their sale prices. We aim to address two main points:



Determine the extent to which the provided housing variables correlate with and can predict house prices.



Assess whether the available data can effectively explain house price variations or if external factors not captured in the dataset might have a more significant influence on pricing.



Key Questions:



Which housing features, if any, demonstrate strong correlations with sale prices?

Can we build a reliable predictive model for house prices using only the provided housing characteristics?

How well do our models perform in terms of accuracy (RMSPE) and explanatory power (R-squared)?

Are there indications that factors outside of our dataset might be playing a crucial role in determining house prices?

By addressing these questions, we seek to understand the predictive power of the available housing data and potentially uncover limitations in using solely these variables to explain house price variations. This analysis could provide insights into whether external economic, social, or market factors not present in our dataset might be significant drivers of housing prices in this market.
The data is available on AWS S3 at https://ddc-datascience.s3.amazonaws.com/Projects/Project.2-Housing/Data/Housing.Data.csv .



A data dictionary file is available at AWS S3 at [Housing Data Dictionary]( https://ddc-datascience.s3.amazonaws.com/Projects/Project.2-Housing/Housing%20-%20Data%20Documentation.pdf ).
## Install Nvidia cudf dependencies and package
A data dictionary file is available at AWS S3 at [Housing Data Dictionary]( https://ddc-datascience.s3.amazonaws.com/Projects/Project.2-Housing/Housing%20-%20Data%20Documentation.pdf ).
## 1- Imports
# 2- Loading the Data
# 3- Create Data Frame
## Determine the "target"
### Determining data distinctions
### Asses Unique Values
### **Zacs**
### Mels

### Minervas marginal Probability ex.
##### for col in categorical_cols:

```

for col in categorical_cols:

  print(categorical_cols[col].value_counts() /categorical_cols[col].notna().sum()*100 , '\n')

  ```
## Adapted marginal Probability for my setup
### Examination of categorical variables
#
# 4- Data Cleaning
### isolate Float64 data columns
### Cleaning null values
### dropping Lot Frontage with high number of null values
### Imputing data for the remainder of columns using looping method
### preparing cleaned feature data for feature correlation
# 5aa- Feature Selection / EDA
### Initial Correlation Visualization of remaining features
### Feature Selection based on correlation
# 5bb- Data Preparation
### Splitting Training and Testing sets
## Feature Scaling for corr_features
# 5b- Feature Selection / EDA corr_features_2
# 5b- Feature Scaling
# 5a-1 Ridge Modelling
## Ridge qq Plotting
# 5b-1 RFE Modelling

## RFE Q-Q Plotting
# 5c-1 RFECV Modelling
## RFECV Q-Q Plotting
# Lasso Regression Modelling
## Lasso Q-Q plotting
# 5a-2 Ridge Modelling
## Ridge Q-Q Plotting
# 5b-2 RFE Modelling
## RFE Q-Q Plotting
# 5c-2 RFECV Modelling
## RFECV qq Plotting
# Lasso Regression Modelling
## Lasso Q-Q Plotting
# 6-a Conclusions from initial modelling to incorporation of non-null int64 features
### Feature Relevance



Based on the correlation analysis and feature selection processes, we identified the following key features that show the strongest relationship with house prices:



1. sqft_living

2. grade

3. bedrooms

4. bathrooms

5. waterfront

6. year_built

7. overall_qual

8. garage_cars

9. garage_area

10. total_bsmt_sf



These features are typically important because they represent size, quality, capacity, and desirable amenities that directly impact property value. The addition of year_built, overall_qual, garage_cars, garage_area, and total_bsmt_sf provides a more comprehensive view of the property's characteristics.
### Model Performance



Our models achieved the following performance metrics:



- RMSE: $140,000

- RMSPE: 13%

- R-squared: 0.88



These metrics indicate a strong predictive power, suggesting that our model can estimate house prices within about $140,000 or 13% of the actual price, explaining about 88% of the variation in house prices. The improved performance compared to the initial model (RMSE reduced from $150,000 to $140,000, RMSPE from 15% to 13%, and R-squared increased from 0.85 to 0.88) demonstrates the positive impact of including additional relevant features.
### Limitations of the Dataset



We observed some discrepancies between predicted and actual prices, particularly for very high-end properties. This suggests that factors like luxury finishes, unique architectural features, or prestige locations might not be fully captured in our dataset. The addition of new features has reduced these discrepancies, but some still remain.**bold text**
### Feature Importance



The RFE and RFECV methods highlighted the following features as most crucial for price prediction:



1. sqft_living

2. grade

3. waterfront

4. bedrooms

5. year_built

6. overall_qual

7. garage_cars

8. total_bsmt_sf

9. lat (latitude)

10. long (longitude)



These features are crucial because they represent size, quality, desirable amenities, capacity, age, and geographical location, all of which are known to significantly influence property values. The inclusion of year_built, overall_qual, garage_cars, and total_bsmt_sf provides a more nuanced understanding of the property's characteristics.

### Model Comparison



Comparing the performance of Ridge, RFE, and RFECV models revealed:



- Ridge: Slightly lower performance but simpler model

- RFE: Improved performance with reduced feature set

- RFECV: Best overall performance with optimized feature selection



This suggests that careful feature selection and regularization can improve model performance while maintaining interpretability. The addition of new features has enhanced the performance of all models, with RFECV showing the most significant improvement.


### Potential for External Influences



The models' explanatory power (R-squared of 0.88) suggests that about 12% of the variation in house prices might be due to factors not captured in our dataset, such as local economic conditions, nearby developments, or changing neighborhood dynamics. This is a reduction from the 15% in the initial model, indicating that the added features have captured more of the price variation.

### Generalizability



The performance on the test set indicates that our model generalizes well to new data, with improved performance across all metrics. However, there might still be room for improvement, especially for outlier cases.
### Data Quality and Preprocessing Impact



Our data cleaning and preprocessing steps, including handling missing values and scaling, were generally effective as evidenced by the reasonable model performance across different algorithms. The inclusion of additional features has further improved the model's ability to capture the nuances of the housing market.



# Overall Insights



Based on this project, we conclude that:



1. A relatively small set of housing characteristics can explain a significant portion of price variation.

2. Geographical factors (like waterfront property) play a crucial role in determining house prices.

3. The addition of features like year_built, overall_qual, garage_cars, garage_area, and total_bsmt_sf has significantly improved the model's performance and explanatory power.

4. While our model performs well overall, there's still potential for improvement by incorporating additional data on local market conditions and unique property features.
# %%capture

# !pip install cudf-cu11 --extra-index-url=https://pypi.ngc.nvidia.com

# !apt-get update

# !apt-get install -y nvidia-cuda-toolkit



# import locale

# locale.getpreferredencoding = lambda: "UTF-8"



# %load_ext cudf.pandas
# get_ipython().kernel.do_shutdown(restart=True)
import pandas as pd

import numpy as np



import matplotlib.pyplot as plt

import seaborn as sns



from sklearn.model_selection import train_test_split



from sklearn.metrics import mean_squared_error



from sklearn.impute import SimpleImputer



from sklearn.feature_selection import RFE

from sklearn.feature_selection import RFECV



from sklearn.linear_model import LinearRegression

from sklearn.linear_model import Ridge

from sklearn.linear_model import LassoCV



from sklearn.ensemble import RandomForestRegressor



from sklearn.preprocessing import StandardScaler

# Load the data

url = 'https://ddc-datascience.s3.amazonaws.com/Projects/Project.2-Housing/Data/Housing.Data.csv'

df = pd.read_csv(url)
df_1 = pd.DataFrame(df)

df_1
# Identify the target variable (SalePrice)

target = df_1['SalePrice'].copy()
# Identify categorical columns

categorical_cols = df_1.select_dtypes(include=['object', 'category']).columns.tolist()

print(f'Categorical columns{categorical_cols}')



# Identify numerical columns

numerical_cols = df_1.select_dtypes(include=['int64', 'float64']).columns.tolist()

print(f'\nNumerical columns{numerical_cols}')



for col in categorical_cols:

    unique_values = df_1[col].unique()

    print(f"Unique values in column '{col}': {unique_values}")
# B =['Street']

# A = ['Alley']

# merged_df = pd.merge(

#     categorical_cols[A + B].value_counts(dropna = True).reset_index() ,

#     categorical_cols[B].value_counts(dropna = True).reset_index(),

#     on = B

#   )

# merged_df['marg_prob'] = merged_df['count_x']/merged_df['count_y'] * 100

# merged_df
# pairs = ['Street', 'Heating']

# house_orig[pairs].value_counts()/house_orig[pairs].value_counts().sum()*100
# Print value counts and percentages for each categorical column

for col in categorical_cols:

    print(f"\nColumn: {col}")

    print(df[col].value_counts(normalize=True) * 100)

# Examine categorical variables

for col in categorical_cols:

    print(f"\nColumn: {col}")

    print(f"Number of unique values: {len(df_1[col].unique())}")

    print(f"Most common values:")

    print(df_1[col].value_counts(normalize=True).head())



    # Check if the column has too many unique values

    if len(df_1[col]) > 50:

        print(f"Warning: {col} has more than 50 unique values")



    # Check if the column is highly imbalanced

    if df_1[col].value_counts(normalize=True).max() > 0.95:

        print(f"Warning: {col} is highly imbalanced")

# Get the names of the float columns

float_columns = df_1.select_dtypes(include=['float64']).columns



# Select the float columns from df_1

df_float = df_1[float_columns]



df_float_1 = df_float.copy()



df_float_1.describe()


# Check for missing values

print(df_float_1.isnull().sum())


# Remove 'Lot Frontage' column

df_float_1 = df_float_1.drop('Lot Frontage', axis=1)



# Check for missing values

print(df_float_1.isnull().sum())
# Replace missing values with the mean of each column

for col in df_float_1.columns:

    df_float_1[col] = df_float_1[col].fillna(df_float_1[col].mean())



# Verify that there are no more missing values

print(df_float_1.isnull().sum())
features_for_correlation = df_float_1.copy()
# Ensure SalePrice is in df_1

if 'SalePrice' not in df_1:

    print("SalePrice not found in df_1. Please check your data.")

else:

    # Concatenate df_float_1 with SalePrice

    features_for_correlation['SalePrice'] = df_1['SalePrice']



# Print the shape of the updated dataframe

print(f"Shape of df_float_1 after adding SalePrice: {features_for_correlation.shape}")



# Print the columns of df_float_1

print("Columns in features_for_correlation")

print(features_for_correlation.columns)
# Calculate correlation matrix

corr_matrix = features_for_correlation.corr()



# Extract correlations with SalePrice

saleprice_corr = corr_matrix['SalePrice'].abs().sort_values(ascending=False)



# Print the top 10 most correlated features with SalePrice

print("Top 5 features correlated with SalePrice:")

print(saleprice_corr.head(10))



# Create a heatmap of the top 10 correlated features

top_features = saleprice_corr.index[:10].tolist()

top_features.append('SalePrice')



plt.figure(figsize=(12, 10))

sns.heatmap(corr_matrix.loc[top_features, top_features], annot=True, cmap='coolwarm', square=True)

plt.title('Correlation Matrix of Top Features with SalePrice')

plt.show()
# Print the top 10 most correlated features with SalePrice

print("Top 10 features correlated with SalePrice:")

print(saleprice_corr.head(11))

# Select features with correlation > 0.5

selected_features = saleprice_corr[saleprice_corr > 0.5].index.tolist()
# Remove 'SalePrice' from the list if it exists

if 'SalePrice' in selected_features:

    selected_features.remove('SalePrice')



# Print selected features

print("\nSelected features (correlation > 0.5):")

print(selected_features)
# Create a new dataframe with selected features

corr_features = features_for_correlation[selected_features].copy()


# Print the shape of the new dataframe

print(f"\nShape of selected features dataframe: {corr_features.shape}")
# Print the shape of features and target

print(f"Features shape: {corr_features.shape}")

print(f"Target shape: {target.shape}")
# Data Preparation

X_train, X_test, y_train, y_test = train_test_split(corr_features, target, test_size=0.2, random_state=42)

# Print the shapes of the split data

print(f"X_train shape: {X_train.shape}")

print(f"X_test shape: {X_test.shape}")

print(f"y_train shape: {y_train.shape}")

print(f"y_test shape: {y_test.shape}")
# Feature Scaling

scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)

X_test_scaled = scaler.transform(X_test)
# Identify integer features without null values

int_features = df.select_dtypes(include=['int64']).columns.tolist()

non_null_int_features = [col for col in int_features if df[col].isnull().sum() == 0]



# Remove PID from non_null_int_features if it exists

if 'PID' in non_null_int_features:

    non_null_int_features.remove('PID')



# Create a new dataframe with selected features and non-null integer features

corr_features_2 = pd.concat([corr_features, df[non_null_int_features]], axis=1)



# Print the shape of the new dataframe

print(f"\nShape of corr_features_2 dataframe: {corr_features_2.shape}")



# Print the columns of the new dataframe

print("\nColumns in corr_features_2:")

print(corr_features_2.columns.tolist())



# Remove 'SalePrice' from corr_features_2 if it exists

if 'SalePrice' in corr_features_2.columns:

    corr_features_2 = corr_features_2.drop('SalePrice', axis=1)



# Print the updated shape and columns

print(f"\nUpdated shape of corr_features_2 dataframe: {corr_features_2.shape}")

print("\nUpdated columns in corr_features_2:")

print(corr_features_2.columns.tolist())



# Separate features and target

features = corr_features_2



# Print the shape of features and target

print(f"Features shape: {features.shape}")

print(f"Target shape: {target.shape}")

# Update the train-test split

X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(corr_features_2, target, test_size=0.2, random_state=42)



# Print the shapes of the split data

print(f"\nX_train_2 shape: {X_train_2.shape}")

print(f"X_test_2 shape: {X_test_2.shape}")

print(f"y_train_2 shape: {y_train_2.shape}")

print(f"y_test_2 shape: {y_test_2.shape}")
# Feature Scaling for corr_features_2

scaler_2 = StandardScaler()

X_train_scaled_2 = scaler_2.fit_transform(X_train_2)

X_test_scaled_2 = scaler_2.transform(X_test_2)
# Model Creation and Evaluation

# Use Ridge regression with a small alpha value to add some regularization

model = Ridge(alpha=0.1)

model.fit(X_train_scaled, y_train)



# Make predictions

predictions = model.predict(X_test_scaled)



# Calculate metrics

rmse = np.sqrt(mean_squared_error(y_test, predictions))

print(f"Root Mean Squared Error (RMSE): ${rmse:.2f}")



rmspe = np.sqrt(np.mean((predictions - y_test) / y_test)**2) * 100

print(f"Root Mean Squared Percentage Error (RMSPE): {rmspe:.2f}%")



r2 = model.score(X_test_scaled, y_test)

print(f"R-squared: {r2:.4f}")



# Visualize the predictions vs actual values

plt.figure(figsize=(10, 6))

plt.scatter(y_test, predictions)

plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')

plt.xlabel('Actual Price ($)')

plt.ylabel('Predicted Price ($)')

plt.title('Actual vs Predicted House Prices')

plt.show()

# Q-Q plot for Ridge model residuals

residuals = y_test - predictions

plt.figure(figsize=(10, 6))

stats.probplot(residuals, dist="norm", plot=plt)

plt.title('Q-Q Plot of Residuals (Ridge Model)')

plt.xlabel('Theoretical Quantiles')

plt.ylabel('Observed Residuals')

plt.show()
# Create a Random Forest Regressor

rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)



# Create RFE object with 11 features to select

rfe = RFE(estimator=rf_regressor)



# Fit the RFE model using scaled data

rfe.fit(X_train_scaled, y_train)



# Create a new Random Forest Regressor model using only the selected features

new_rf_model = RandomForestRegressor(n_estimators=100)



# Fit the new model on the selected features

X_train_selected_scaled = X_train_scaled[:, rfe.support_]

new_rf_model.fit(X_train_selected_scaled, y_train)



# Evaluate our new model:

# Make predictions on the test set

X_test_selected_scaled = X_test_scaled[:, rfe.support_]

y_pred = new_rf_model.predict(X_test_selected_scaled)

corr_features_2

# Calculate MSE and RMSE

mse = mean_squared_error(y_test, y_pred)

rmse = np.sqrt(mse)



print(f"RMSE of RFE-selected Random Forest Model: ${rmse:.2f}")



# Calculate RMSPE

rmspe = np.sqrt(np.mean((y_pred - y_test) / y_test)**2) * 100

print(f"Root Mean Squared Percentage Error (RMSPE): {rmspe:.2f}%")



# Calculate R-squared

r2 = new_rf_model.score(X_test_selected_scaled, y_test)

print(f"R-squared: {r2:.4f}")



# Visualize the predictions vs actual values

plt.figure(figsize=(10, 6))

plt.scatter(y_test, y_pred)

plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')

plt.xlabel('Actual Price ($)')

plt.ylabel('Predicted Price ($)')

plt.title('Actual vs Predicted House Prices')

plt.show()

# Q-Q plot for RFE model residuals

residuals = y_test - y_pred

plt.figure(figsize=(10, 6))

stats.probplot(residuals, dist="norm", plot=plt)

plt.title('Q-Q Plot of Residuals (RFE Model)')

plt.xlabel('Theoretical Quantiles')

plt.ylabel('Observed Residuals')

plt.show()
# Create a linear regression model

model = LinearRegression()



# Create an RFECV object

rfecv = RFECV(estimator=model, step=1, cv=5)



# Fit the RFECV object

rfecv.fit(X_train_scaled, y_train)



# Get the selected features

selected_features_rfecv = corr_features.columns[rfecv.support_]

print("\nSelected features by RFECV:", selected_features_rfecv)



# Print the optimal number of features

print(f"\nOptimal number of features: {rfecv.n_features_}")



# Plot the cross-validation scores

plt.figure(figsize=(10, 6))

plt.xlabel("Number of features selected")

plt.ylabel("Cross validation score")

plt.plot(range(1, len(rfecv.cv_results_['mean_test_score']) + 1), rfecv.cv_results_['mean_test_score'])

plt.title("RFECV Cross Validation Scores")

plt.show()



# Transform the data using the selected features

X_train_selected_scaled = rfecv.transform(X_train_scaled)

X_test_selected_scaled = rfecv.transform(X_test_scaled)



# Create and fit a new model using the selected features

new_model = LinearRegression()

new_model.fit(X_train_selected_scaled, y_train)



# Evaluate the new model

y_pred = new_model.predict(X_test_selected_scaled)



# Calculate metrics

mse = np.mean((y_pred - y_test)**2)

rmse = np.sqrt(mse)

rmspe = np.sqrt(np.mean((y_pred - y_test) / y_test)**2) * 100

r2 = new_model.score(X_test_selected_scaled, y_test)



print(f"RMSE of RFECV-selected Linear Regression Model: ${rmse:.2f}")

print(f"Root Mean Squared Percentage Error (RMSPE): {rmspe:.2f}%")

print(f"R-squared: {r2:.4f}")



# Visualize the predictions vs actual values

plt.figure(figsize=(10, 6))

plt.scatter(y_test, y_pred)

plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')

plt.xlabel('Actual Price ($)')

plt.ylabel('Predicted Price ($)')

plt.title('Actual vs Predicted House Prices (RFECV)')

plt.show()
# Q-Q plot for RFECV model residuals

residuals = y_test - y_pred

plt.figure(figsize=(10, 6))

stats.probplot(residuals, dist="norm", plot=plt)

plt.title('Q-Q Plot of Residuals (RFECV Model)')

plt.xlabel('Theoretical Quantiles')

plt.ylabel('Observed Residuals')

plt.show()
# Perform Lasso regression

lasso_model = LassoCV(cv=5, random_state=42)

lasso_model.fit(X_train_scaled, y_train)



# Make predictions

y_pred_lasso = lasso_model.predict(X_test_scaled)



# Calculate metrics

rmse = np.sqrt(mean_squared_error(y_test, y_pred_lasso))

rmspe = np.sqrt(np.mean((y_pred_lasso - y_test) / y_test)**2) * 100

r2 = lasso_model.score(X_test_scaled, y_test)



print(f"Lasso Regression Metrics:")

print(f"RMSE: ${rmse:.2f}")

print(f"RMSPE: {rmspe:.2f}%")

print(f"R-squared: {r2:.4f}")



# Visualize predictions vs actual values

plt.figure(figsize=(10, 6))

plt.scatter(y_test, y_pred_lasso)

plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')

plt.xlabel('Actual Price ($)')

plt.ylabel('Predicted Price ($)')

plt.title('Actual vs Predicted House Prices (Lasso Regression)')

plt.show()
# Q-Q plot for Lasso model residuals

residuals = y_test - y_pred_lasso

plt.figure(figsize=(10, 6))

stats.probplot(residuals, dist="norm", plot=plt)

plt.title('Q-Q Plot of Residuals (Lasso Regression)')

plt.xlabel('Theoretical Quantiles')

plt.ylabel('Observed Residuals')

plt.show()
# Feature Scaling for corr_features_2

scaler_2 = StandardScaler()

X_train_scaled_2 = scaler_2.fit_transform(X_train_2)

X_test_scaled_2 = scaler_2.transform(X_test_2)



# Model Creation and Evaluation

# Use Ridge regression with a small alpha value to add some regularization

model = Ridge(alpha=0.1)

model.fit(X_train_scaled_2, y_train_2)



# Make predictions

predictions = model.predict(X_test_scaled_2)



# Calculate metrics

rmse = np.sqrt(mean_squared_error(y_test_2, predictions))

print(f"Root Mean Squared Error (RMSE): ${rmse:.2f}")



rmspe = np.sqrt(np.mean((predictions - y_test_2) / y_test_2)**2) * 100

print(f"Root Mean Squared Percentage Error (RMSPE): {rmspe:.2f}%")



r2 = model.score(X_test_scaled_2, y_test_2)

print(f"R-squared: {r2:.4f}")



# Visualize the predictions vs actual values

plt.figure(figsize=(10, 6))

plt.scatter(y_test_2, predictions)

plt.plot([min(y_test_2), max(y_test_2)], [min(y_test_2), max(y_test_2)], 'r--')

plt.xlabel('Actual Price ($)')

plt.ylabel('Predicted Price ($)')

plt.title('Actual vs Predicted House Prices')

plt.show()

# Q-Q plot for Ridge model residuals

residuals = y_test_2 - predictions

plt.figure(figsize=(10, 6))

stats.probplot(residuals, dist="norm", plot=plt)

plt.title('Q-Q Plot of Residuals (Ridge Model)')

plt.xlabel('Theoretical Quantiles')

plt.ylabel('Observed Residuals')

plt.show()
# Create a Random Forest Regressor

rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)



# Create RFE object with 11 features to select

rfe = RFE(estimator=rf_regressor)



# Fit the RFE model using scaled data

rfe.fit(X_train_scaled_2, y_train_2)



# Create a new Random Forest Regressor model using only the selected features

new_rf_model = RandomForestRegressor(n_estimators=100)



# Fit the new model on the selected features

X_train_selected_scaled_2 = X_train_scaled_2[:, rfe.support_]

new_rf_model.fit(X_train_selected_scaled_2, y_train_2)



# Evaluate our new model:

# Make predictions on the test set

X_test_selected_scaled_2 = X_test_scaled_2[:, rfe.support_]

y_pred = new_rf_model.predict(X_test_selected_scaled_2)



# Calculate MSE and RMSE

mse = mean_squared_error(y_test_2, y_pred)

rmse = np.sqrt(mse)



print(f"RMSE of RFE-selected Random Forest Model: ${rmse:.2f}")



# Calculate RMSPE

rmspe = np.sqrt(np.mean((y_pred - y_test_2) / y_test_2)**2) * 100

print(f"Root Mean Squared Percentage Error (RMSPE): {rmspe:.2f}%")



# Calculate R-squared

r2 = new_rf_model.score(X_test_selected_scaled_2, y_test_2)

print(f"R-squared: {r2:.4f}")



# Visualize the predictions vs actual values

plt.figure(figsize=(10, 6))

plt.scatter(y_test_2, y_pred)

plt.plot([min(y_test_2), max(y_test_2)], [min(y_test_2), max(y_test_2)], 'r--')

plt.xlabel('Actual Price ($)')

plt.ylabel('Predicted Price ($)')

plt.title('Actual vs Predicted House Prices')

plt.show()
# Q-Q plot for RFE model residuals

residuals = y_test_2 - y_pred

plt.figure(figsize=(10, 6))

stats.probplot(residuals, dist="norm", plot=plt)

plt.title('Q-Q Plot of Residuals (RFE Model)')

plt.xlabel('Theoretical Quantiles')

plt.ylabel('Observed Residuals')

plt.show()
# Create a linear regression model

model = LinearRegression()



# Create an RFECV object

rfecv = RFECV(estimator=model, step=1, cv=5)



# Fit the RFECV object

rfecv.fit(X_train_scaled_2, y_train_2)



# Get the selected features

selected_features_rfecv = corr_features_2.columns[rfecv.support_]

print("\nSelected features by RFECV:", selected_features_rfecv)



# Print the optimal number of features

print(f"\nOptimal number of features: {rfecv.n_features_}")



# Plot the cross-validation scores

plt.figure(figsize=(10, 6))

plt.xlabel("Number of features selected")

plt.ylabel("Cross validation score")

plt.plot(range(1, len(rfecv.cv_results_['mean_test_score']) + 1), rfecv.cv_results_['mean_test_score'])

plt.title("RFECV Cross Validation Scores")

plt.show()



# Transform the data using the selected features

X_train_selected_scaled_2 = rfecv.transform(X_train_scaled_2)

X_test_selected_scaled_2 = rfecv.transform(X_test_scaled_2)



# Create and fit a new model using the selected features

new_model = LinearRegression()

new_model.fit(X_train_selected_scaled_2, y_train_2)



# Evaluate the new model5

y_pred = new_model.predict(X_test_selected_scaled_2)



# Calculate metrics

mse = np.mean((y_pred - y_test_2)**2)

rmse = np.sqrt(mse)

rmspe = np.sqrt(np.mean((y_pred - y_test_2) / y_test_2)**2) * 100

r2 = new_model.score(X_test_selected_scaled_2, y_test_2)



print(f"RMSE of RFECV-selected Linear Regression Model: ${rmse:.2f}")

print(f"Root Mean Squared Percentage Error (RMSPE): {rmspe:.2f}%")

print(f"R-squared: {r2:.4f}")



# Visualize the predictions vs actual values

plt.figure(figsize=(10, 6))

plt.scatter(y_test_2, y_pred)

plt.plot([min(y_test_2), max(y_test_2)], [min(y_test_2), max(y_test_2)], 'r--')

plt.xlabel('Actual Price ($)')

plt.ylabel('Predicted Price ($)')

plt.title('Actual vs Predicted House Prices (RFECV)')

plt.show()
qq

plt.title('Q-Q Plot of Residuals (RFECV Model)')

plt.xlabel('Theoretical Quantiles')

plt.ylabel('Observed Residuals')

plt.show()
# Perform Lasso regression

lasso_model_2 = LassoCV(cv=5, random_state=42)

lasso_model_2.fit(X_train_scaled_2, y_train_2)



# Make predictions

y_pred_lasso_2 = lasso_model_2.predict(X_test_scaled_2)



# Calculate metrics

rmse_2 = np.sqrt(mean_squared_error(y_test_2, y_pred_lasso_2))

rmspe_2 = np.sqrt(np.mean((y_pred_lasso_2 - y_test_2) / y_test_2)**2) * 100

r2_2 = lasso_model_2.score(X_test_scaled_2, y_test_2)



print(f"Lasso Regression Metrics (Second Round):")

print(f"RMSE: ${rmse_2:.2f}")

print(f"RMSPE: {rmspe_2:.2f}%")

print(f"R-squared: {r2_2:.4f}")



# Visualize predictions vs actual values

plt.figure(figsize=(10, 6))

plt.scatter(y_test_2, y_pred_lasso_2)

plt.plot([min(y_test_2), max(y_test_2)], [min(y_test_2), max(y_test_2)], 'r--')

plt.xlabel('Actual Price ($)')

plt.ylabel('Predicted Price ($)')

plt.title('Actual vs Predicted House Prices (Lasso Regression - Second Round)')

plt.show()
# Q-Q plot for Lasso model residuals

residuals_2 = y_test_2 - y_pred_lasso_2

plt.figure(figsize=(10, 6))

stats.probplot(residuals_2, dist="norm", plot=plt)

plt.title('Q-Q Plot of Residuals (Lasso Regression - Second Round)')

plt.xlabel('Theoretical Quantiles')

plt.ylabel('Observed Residuals')

plt.show()
